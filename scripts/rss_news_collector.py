#!/usr/bin/env python3
"""
åŸºäº RSS è®¢é˜…çš„æ–°é—»æ”¶é›†è„šæœ¬
ä½¿ç”¨ Python å†…ç½®åº“è§£æ RSSï¼Œæ— éœ€é¢å¤–ä¾èµ–
"""

import os
import sys
import json
import urllib.request
import urllib.error
import ssl
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from typing import List, Dict
import re
from email.utils import parsedate_to_datetime
from zhdate import ZhDate

# åˆ›å»º SSL ä¸Šä¸‹æ–‡ï¼ˆå¤„ç†è¯ä¹¦é—®é¢˜ï¼‰
ssl_context = ssl._create_unverified_context()

# é…ç½®
DOUBAO_API_KEY = os.environ.get("DOUBAO_API_KEY")
OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY")
# å·¥ä½œç›®å½• - å…¼å®¹æœ¬åœ°å’Œ GitHub Actions
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
WORK_DIR = os.path.dirname(SCRIPT_DIR)
LOG_FILE = os.path.join(WORK_DIR, "logs", "rss-news.log")

# æ£€æŸ¥ API Key - ä¼˜å…ˆä½¿ç”¨ OpenRouterï¼ˆGitHub Actions æ›´ç¨³å®šï¼‰ï¼Œå¤‡ç”¨è±†åŒ…
if not OPENROUTER_API_KEY and not DOUBAO_API_KEY:
    print("é”™è¯¯: æœªè®¾ç½® OPENROUTER_API_KEY æˆ– DOUBAO_API_KEY ç¯å¢ƒå˜é‡")
    print("è¯·è¿è¡Œ: export OPENROUTER_API_KEY='your-api-key'")
    print("æˆ–è€…: export DOUBAO_API_KEY='your-api-key'")
    sys.exit(1)

# ç¡®å®šä½¿ç”¨å“ªä¸ª API
USE_OPENROUTER = bool(OPENROUTER_API_KEY)

# RSS æºé…ç½®ï¼ˆä¼˜åŒ–åï¼šä¼˜å…ˆä½¿ç”¨ä» GitHub Actions ç¾å›½æœåŠ¡å™¨èƒ½ç¨³å®šè®¿é—®çš„æºï¼‰
ALL_RSS_SOURCES = [
    # å›½å†…æºï¼ˆä»ç¾å›½è®¿é—®è¾ƒç¨³å®šçš„ï¼‰
    {"name": "æœºå™¨ä¹‹å¿ƒ", "url": "https://www.jiqizhixin.com/rss", "limit": 10},
    {"name": "36æ°ª", "url": "https://36kr.com/feed", "limit": 10},
    {"name": "è™å—…", "url": "https://www.huxiu.com/rss/0.xml", "limit": 8},
    {"name": "é’›åª’ä½“", "url": "https://www.tmtpost.com/rss", "limit": 8},
    # å›½é™…æºï¼ˆä»ç¾å›½è®¿é—®ç¨³å®šï¼‰
    {"name": "TechCrunch", "url": "https://techcrunch.com/feed/", "limit": 8},
    {"name": "TechCrunch AI", "url": "https://techcrunch.com/category/artificial-intelligence/feed/", "limit": 8},
    {"name": "The Verge", "url": "https://www.theverge.com/rss/index.xml", "limit": 8},
    {"name": "Wired", "url": "https://www.wired.com/feed/rss", "limit": 5},
    {"name": "Ars Technica", "url": "https://feeds.arstechnica.com/arstechnica/index", "limit": 5},
    {"name": "Reuters Tech", "url": "https://www.reutersagency.com/feed/?taxonomy=best-topics&post_type=best", "limit": 5},
]

# ç›®æ ‡åˆ†ç±»
CATEGORIES = ["AI é¢†åŸŸ", "ç§‘æŠ€åŠ¨æ€", "è´¢ç»è¦é—»"]

def log(message):
    """è®°å½•æ—¥å¿—"""
    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{timestamp}] {message}\n")
    print(message)

def fetch_rss_items(url: str, limit: int = 10, hours_ago: int = 48) -> List[Dict]:
    """è·å– RSS æ¡ç›®"""
    try:
        # è®¾ç½®ç”¨æˆ·ä»£ç†
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }

        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=30, context=ssl_context) as response:
            content = response.read()

        # è§£æ XML
        root = ET.fromstring(content)

        # RSS æ ¼å¼ï¼š//rss/channel/item æˆ– //feed/entry
        items = []
        namespaces = {'': ''}  # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ å‘½åç©ºé—´

        # å°è¯•ä¸åŒçš„ RSS/Atom æ ¼å¼
        item_elements = root.findall('.//item') or root.findall('.//{http://www.w3.org/2005/Atom}entry')

        # ç²¾ç¡®çš„æ—¶é—´è¿‡æ»¤ï¼šåªæ”¶é›†æ˜¨å¤©ä¸€æ•´å¤©çš„æ–°é—»
        now = datetime.now()
        yesterday_start = datetime(now.year, now.month, now.day) - timedelta(days=1)  # æ˜¨å¤© 00:00:00
        yesterday_end = datetime(now.year, now.month, now.day) - timedelta(seconds=1)  # æ˜¨å¤© 23:59:59

        # å¤‡ç”¨ï¼šå¦‚æœéœ€è¦æ›´å®½æ¾çš„æ—¶é—´çª—å£ï¼ˆè¿‡å»24å°æ—¶ï¼‰
        cutoff_time = datetime.now() - timedelta(hours=hours_ago)

        for elem in item_elements[:limit * 2]:
            item = {}

            # æ ‡é¢˜ - æ›´å¥å£®çš„è§£æ
            title_text = ''
            for title_path in ['title', '{http://www.w3.org/2005/Atom}title']:
                title_elem = elem.find(title_path)
                if title_elem is not None and title_elem.text:
                    title_text = title_elem.text.strip()
                    break
            item['title'] = title_text if title_text else 'æ— æ ‡é¢˜'

            # æè¿°/æ‘˜è¦
            desc_text = ''
            for desc_path in ['description', '{http://www.w3.org/2005/Atom}summary', 'content', '{http://www.w3.org/2005/Atom}content']:
                desc_elem = elem.find(desc_path)
                if desc_elem is not None and desc_elem.text:
                    desc_text = desc_elem.text
                    break
            # ç§»é™¤ HTML æ ‡ç­¾
            desc_text = re.sub('<[^<]+?>', '', desc_text)
            desc_text = desc_text.strip()
            item['summary'] = desc_text[:500] if desc_text else ''

            # é“¾æ¥
            link_text = ''
            for link_path in ['link', '{http://www.w3.org/2005/Atom}link']:
                link_elem = elem.find(link_path)
                if link_elem is not None:
                    # å°è¯•è·å– href å±æ€§æˆ–æ–‡æœ¬
                    link_text = link_elem.get('href', '') or (link_elem.text if link_elem.text else '')
                    if link_text:
                        break
            item['link'] = link_text

            # å‘å¸ƒæ—¶é—´
            pub_text = ''
            for date_path in ['pubDate', '{http://www.w3.org/2005/Atom}published', 'date']:
                date_elem = elem.find(date_path)
                if date_elem is not None and date_elem.text:
                    pub_text = date_elem.text
                    break
            item['published'] = pub_text

            # æ¥æº
            source_text = ''
            for source_path in ['.//title', './/{http://www.w3.org/2005/Atom}title']:
                source_elem = root.find(source_path)
                if source_elem is not None and source_elem.text:
                    source_text = source_elem.text
                    break
            item['source'] = source_text if source_text else 'æœªçŸ¥æ¥æº'

            # ç²¾ç¡®çš„æ—¶é—´æ£€æŸ¥ï¼šåªæ”¶é›†æ˜¨å¤©çš„æ–°é—»
            if item['published']:
                try:
                    pub_time = parsedate_to_datetime(item['published'])
                    # è½¬æ¢ä¸ºæœ¬åœ°æ—¶åŒºï¼ˆåŒ—äº¬æ—¶é—´ï¼‰è¿›è¡Œæ¯”è¾ƒ
                    pub_time_local = pub_time.astimezone()

                    # æå–æ—¥æœŸéƒ¨åˆ†è¿›è¡Œæ¯”è¾ƒï¼ˆå¿½ç•¥å…·ä½“æ—¶é—´ï¼‰
                    pub_date = pub_time_local.date()
                    yesterday_date = yesterday_start.date()

                    # åªä¿ç•™æ˜¨å¤©çš„æ–°é—»
                    if pub_date != yesterday_date:
                        continue

                    # è®°å½•æ–°é—»çš„å‘å¸ƒæ—¶é—´ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                    item['parsed_time'] = pub_time_local.strftime("%Y-%m-%d %H:%M:%S")
                except:
                    # æ— æ³•è§£ææ—¶é—´ï¼Œä½¿ç”¨å®½æ¾çš„æ—¶é—´çª—å£ï¼ˆè¿‡å»24å°æ—¶ï¼‰
                    # è¿™æ ·å¯ä»¥ç¡®ä¿ä¸ä¼šé—æ¼é‡è¦æ–°é—»
                    pass

            items.append(item)

        return items[:limit]

    except Exception as e:
        log(f"è·å– RSS å¤±è´¥ [{url}]: {e}")
        return []

def collect_all_news() -> List[Dict]:
    """æ”¶é›†æ‰€æœ‰ RSS æ–°é—»åˆ°ä¸€èµ·"""
    all_items = []

    # è®¡ç®—æ—¶é—´èŒƒå›´ç”¨äºæ—¥å¿—
    now = datetime.now()
    yesterday_start = datetime(now.year, now.month, now.day) - timedelta(days=1)
    yesterday_end = datetime(now.year, now.month, now.day) - timedelta(seconds=1)

    log("å¼€å§‹æ”¶é›† RSS æ–°é—»...")
    log(f"æ—¶é—´è¿‡æ»¤èŒƒå›´: {yesterday_start.strftime('%Y-%m-%d %H:%M:%S')} åˆ° {yesterday_end.strftime('%Y-%m-%d %H:%M:%S')}")

    for source in ALL_RSS_SOURCES:
        log(f"  - {source['name']}")
        items = fetch_rss_items(source['url'], source['limit'])
        for item in items:
            item['rss_source'] = source['name']
        all_items.extend(items)
        log(f"    è·å– {len(items)} æ¡")

    # å»é‡ï¼ˆåŸºäºæ ‡é¢˜ï¼‰
    seen_titles = set()
    unique_items = []
    for item in all_items:
        title_lower = item['title'].lower()
        if title_lower not in seen_titles and item['title'] != 'æ— æ ‡é¢˜':
            seen_titles.add(title_lower)
            unique_items.append(item)

    log(f"æ”¶é›†å®Œæˆï¼Œå…±è·å– {len(unique_items)} æ¡å»é‡åæ–°é—»")
    return unique_items

def classify_news_with_ai(news_items: List[Dict]) -> Dict[str, List[Dict]]:
    """ä½¿ç”¨ AI å°†æ–°é—»åˆ†ç±»åˆ° 3 ä¸ªç±»åˆ«"""
    log("æ­£åœ¨ä½¿ç”¨ AI åˆ†ç±»æ–°é—»...")

    # å‡†å¤‡æ–°é—»åˆ—è¡¨ï¼ˆæœ€å¤š30æ¡ï¼Œé¿å… token è¿‡å¤šï¼‰
    news_list = news_items[:30]

    # æ„å»ºåˆ†ç±» prompt
    news_text = ""
    for i, item in enumerate(news_list, 1):
        news_text += f"{i}. æ ‡é¢˜: {item['title']}\n"
        if item['summary']:
            news_text += f"   æ‘˜è¦: {item['summary'][:100]}\n"
        news_text += f"   æ¥æº: {item['rss_source']}\n\n"

    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šæ–°é—»ç¼–è¾‘ã€‚è¯·å°†ä»¥ä¸‹æ–°é—»ä¸¥æ ¼åˆ†ç±»åˆ° 3 ä¸ªç±»åˆ«ä¸­ï¼š

{news_text}

åˆ†ç±»æ ‡å‡†ï¼š
- **AI é¢†åŸŸ**: äººå·¥æ™ºèƒ½ã€å¤§æ¨¡å‹ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ã€æœºå™¨äººã€AIåº”ç”¨ç­‰
- **ç§‘æŠ€åŠ¨æ€**: æ™ºèƒ½æ‰‹æœºã€ç”µè„‘ã€èŠ¯ç‰‡ã€äº’è”ç½‘ã€è½¯ä»¶ã€æ¸¸æˆã€æ–°èƒ½æºè½¦ã€èˆªå¤©ã€5G/6Gã€åˆ›ä¸šå…¬å¸ã€äº§å“å‘å¸ƒç­‰ï¼ˆéAIï¼‰
- **è´¢ç»è¦é—»**: è‚¡å¸‚ã€ç»æµã€è´§å¸æ”¿ç­–ã€èèµ„ã€å¹¶è´­ã€IPOã€é‡‘èæ”¿ç­–ã€å®è§‚ç»æµã€ä¼ä¸šè´¢æŠ¥ç­‰

è¯·æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼ˆåªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{
  "AI é¢†åŸŸ": [1, 3, 5, 7, 9],
  "ç§‘æŠ€åŠ¨æ€": [2, 4, 6, 8, 10],
  "è´¢ç»è¦é—»": [11, 12, 13, 14, 15]
}}

æ³¨æ„ï¼š
1. ä¸¥æ ¼æŒ‰ç…§åˆ†ç±»æ ‡å‡†ï¼Œä¸è¦æ··æ·†
2. æ¯ä¸ªç±»åˆ«é€‰æ‹©æœ€é‡è¦çš„ 5 æ¡
3. è¾“å‡ºçº¯ JSON æ ¼å¼"""

    result = call_llm_api(prompt, max_tokens=2000)
    if not result:
        log("AI åˆ†ç±»å¤±è´¥")
        return {"AI é¢†åŸŸ": [], "ç§‘æŠ€åŠ¨æ€": [], "è´¢ç»è¦é—»": []}

    # è§£æ AI è¿”å›çš„ JSON
    try:
        # æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
        result = result.strip()
        if result.startswith('```'):
            result = result.split('\n', 1)[-1]
        if result.endswith('```'):
            result = result.rsplit('\n', 1)[0]
        result = result.strip()

        classification = json.loads(result)

        # æŒ‰åˆ†ç±»ç»„ç»‡æ–°é—»
        categorized = {cat: [] for cat in CATEGORIES}

        for category, indices in classification.items():
            if category in CATEGORIES:
                for idx in indices[:5]:  # æ¯ç±»æœ€å¤š 5 æ¡
                    if idx - 1 < len(news_list):
                        categorized[category].append(news_list[idx - 1])

        log(f"AI åˆ†ç±»å®Œæˆ: AIé¢†åŸŸ{len(categorized['AI é¢†åŸŸ'])}æ¡, ç§‘æŠ€åŠ¨æ€{len(categorized['ç§‘æŠ€åŠ¨æ€'])}æ¡, è´¢ç»è¦é—»{len(categorized['è´¢ç»è¦é—»'])}æ¡")
        return categorized

    except json.JSONDecodeError as e:
        log(f"è§£æ AI åˆ†ç±»ç»“æœå¤±è´¥: {e}")
        log(f"åŸå§‹ç»“æœ: {result[:500]}")
        return {"AI é¢†åŸŸ": [], "ç§‘æŠ€åŠ¨æ€": [], "è´¢ç»è¦é—»": []}

def get_traditional_lunar_date(dt: datetime) -> str:
    """è·å–ä¼ ç»Ÿå†œå†æ—¥æœŸæ ¼å¼ï¼šä¹™å·³å¹´å†¬æœˆå»¿ä¸ƒ"""
    zh_date = ZhDate.from_datetime(dt)

    # è·å–å¤©å¹²åœ°æ”¯å¹´
    chinese_full = zh_date.chinese()
    parts = chinese_full.split()
    gz_year = parts[1] if len(parts) >= 2 else ''

    # å†œå†æœˆä»½ï¼ˆä¼ ç»Ÿå†™æ³•ï¼‰
    months = ['', 'æ­£æœˆ', 'äºŒæœˆ', 'ä¸‰æœˆ', 'å››æœˆ', 'äº”æœˆ', 'å…­æœˆ',
              'ä¸ƒæœˆ', 'å…«æœˆ', 'ä¹æœˆ', 'åæœˆ', 'å†¬æœˆ', 'è…Šæœˆ']
    lunar_month = months[zh_date.lunar_month]

    # å†œå†æ—¥æœŸï¼ˆä¼ ç»Ÿå†™æ³•ï¼‰
    days = ['', 'åˆä¸€', 'åˆäºŒ', 'åˆä¸‰', 'åˆå››', 'åˆäº”', 'åˆå…­', 'åˆä¸ƒ', 'åˆå…«', 'åˆä¹', 'åˆå',
            'åä¸€', 'åäºŒ', 'åä¸‰', 'åå››', 'åäº”', 'åå…­', 'åä¸ƒ', 'åå…«', 'åä¹', 'äºŒå',
            'å»¿ä¸€', 'å»¿äºŒ', 'å»¿ä¸‰', 'å»¿å››', 'å»¿äº”', 'å»¿å…­', 'å»¿ä¸ƒ', 'å»¿å…«', 'å»¿ä¹', 'ä¸‰å']
    lunar_day = days[zh_date.lunar_day]

    return f'{gz_year}{lunar_month}{lunar_day}'

def call_llm_api(prompt: str, max_tokens: int = 4000) -> str:
    """è°ƒç”¨ LLM APIï¼ˆä¼˜å…ˆ OpenRouterï¼Œå¤‡ç”¨è±†åŒ…ï¼‰"""

    if USE_OPENROUTER:
        return call_openrouter_api(prompt, max_tokens)
    else:
        return call_doubao_api(prompt, max_tokens)

def call_openrouter_api(prompt: str, max_tokens: int = 4000) -> str:
    """è°ƒç”¨ OpenRouter APIï¼ˆä» GitHub Actions ç¨³å®šè®¿é—®ï¼‰"""
    url = "https://openrouter.ai/api/v1/chat/completions"

    payload = {
        "model": "openai/gpt-4o-mini",
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "max_tokens": max_tokens,
        "temperature": 0.7
    }

    try:
        data = json.dumps(payload).encode('utf-8')
        req = urllib.request.Request(
            url,
            data=data,
            headers={
                'Authorization': f"Bearer {OPENROUTER_API_KEY}",
                'Content-Type': 'application/json',
                'HTTP-Referer': 'https://github.com/lairulan/daily-tech-news',
                'X-Title': 'Daily Tech News'
            }
        )

        with urllib.request.urlopen(req, timeout=120, context=ssl_context) as response:
            result = json.loads(response.read().decode('utf-8'))
            return result["choices"][0]["message"]["content"]

    except Exception as e:
        log(f"OpenRouter API è°ƒç”¨å¤±è´¥: {e}")
        # å¦‚æœ OpenRouter å¤±è´¥ä¸”æœ‰è±†åŒ… keyï¼Œå°è¯•è±†åŒ…
        if DOUBAO_API_KEY:
            log("å°è¯•ä½¿ç”¨è±†åŒ… API ä½œä¸ºå¤‡ç”¨...")
            return call_doubao_api(prompt, max_tokens)
        return None

def call_doubao_api(prompt: str, max_tokens: int = 4000) -> str:
    """è°ƒç”¨è±†åŒ… APIï¼ˆå¤‡ç”¨ï¼‰"""
    url = "https://ark.cn-beijing.volces.com/api/v3/chat/completions"

    payload = {
        "model": "doubao-seed-1-6-lite-251015",
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "max_tokens": max_tokens,
        "temperature": 0.7
    }

    try:
        data = json.dumps(payload).encode('utf-8')
        req = urllib.request.Request(
            url,
            data=data,
            headers={
                'Authorization': f"Bearer {DOUBAO_API_KEY}",
                'Content-Type': 'application/json'
            }
        )

        with urllib.request.urlopen(req, timeout=120, context=ssl_context) as response:
            result = json.loads(response.read().decode('utf-8'))
            return result["choices"][0]["message"]["content"]

    except Exception as e:
        log(f"è±†åŒ… API è°ƒç”¨å¤±è´¥: {e}")
        return None

def format_news_to_html(categorized_news: Dict[str, List[Dict]], yesterday: str) -> str:
    """å°†åˆ†ç±»åçš„æ–°é—»æ ¼å¼åŒ–ä¸º HTML"""
    # è·å–ä»Šå¤©çš„æ—¥æœŸä¿¡æ¯
    today = datetime.now()
    weekday_names = ["æ˜ŸæœŸä¸€", "æ˜ŸæœŸäºŒ", "æ˜ŸæœŸä¸‰", "æ˜ŸæœŸå››", "æ˜ŸæœŸäº”", "æ˜ŸæœŸå…­", "æ˜ŸæœŸæ—¥"]
    today_weekday = weekday_names[today.weekday()]
    today_date = today.strftime("%Yå¹´%mæœˆ%dæ—¥")

    # è·å–ä¼ ç»Ÿå†œå†æ—¥æœŸ
    today_lunar = get_traditional_lunar_date(today)  # ä¾‹å¦‚: "ä¹™å·³å¹´å†¬æœˆå»¿ä¸ƒ"

    # å‡†å¤‡æ–°é—»æ‘˜è¦
    news_summary = f"ä»¥ä¸‹æ˜¯{yesterday}é€šè¿‡ RSS æ”¶é›†å¹¶åˆ†ç±»çš„æ–°é—»ï¼š\n\n"

    for category in CATEGORIES:
        items = categorized_news.get(category, [])
        news_summary += f"\n## {category}\n"
        for i, item in enumerate(items[:5], 1):
            title = item['title'][:100]
            news_summary += f"{i}. {title}\n"

    # æ„å»º prompt
    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šæ–°é—»ç¼–è¾‘ã€‚ä»¥ä¸‹æ˜¯{yesterday}é€šè¿‡ RSS æ”¶é›†å¹¶åˆ†ç±»çš„æ–°é—»ï¼š

{news_summary}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆåªè¾“å‡º HTMLï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼‰ï¼š

<section style="padding: 20px; font-family: -apple-system, BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', sans-serif;">

<section style="text-align: center; padding: 20px 0 30px 0; background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%); border-radius: 15px; margin-bottom: 30px;">
<p style="margin: 0; font-size: 14px; color: #666; letter-spacing: 1px;">{today_lunar}</p>
<p style="margin: 8px 0 0 0; font-size: 20px; font-weight: bold; color: #333; letter-spacing: 3px;">{today_weekday}</p>
<p style="margin: 8px 0 0 0; font-size: 13px; color: #999;">{today_date}</p>
</section>

<section style="margin-bottom: 30px;">
<p style="display: inline-block; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: #fff; font-size: 18px; font-weight: bold; padding: 10px 25px; border-radius: 25px; margin: 0 0 20px 0; box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);">ğŸ“± AI é¢†åŸŸ</p>
<div style="padding: 0 10px;">
<p style="margin: 0 0 15px 0; line-height: 1.9; color: #333; font-size: 15px;"><span style="color: #667eea; font-weight: bold; margin-right: 8px;">01</span>æ–°é—»å†…å®¹</p>
</div>
</section>

<section style="margin-bottom: 30px;">
<p style="display: inline-block; background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: #fff; font-size: 18px; font-weight: bold; padding: 10px 25px; border-radius: 25px; margin: 0 0 20px 0; box-shadow: 0 4px 15px rgba(245, 87, 108, 0.3);">ğŸ’» ç§‘æŠ€åŠ¨æ€</p>
<div style="padding: 0 10px;">
<p style="margin: 0 0 15px 0; line-height: 1.9; color: #333; font-size: 15px;"><span style="color: #f5576c; font-weight: bold; margin-right: 8px;">01</span>æ–°é—»å†…å®¹</p>
</div>
</section>

<section style="margin-bottom: 30px;">
<p style="display: inline-block; background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); color: #fff; font-size: 18px; font-weight: bold; padding: 10px 25px; border-radius: 25px; margin: 0 0 20px 0; box-shadow: 0 4px 15px rgba(79, 172, 254, 0.3);">ğŸ’° è´¢ç»è¦é—»</p>
<div style="padding: 0 10px;">
<p style="margin: 0 0 15px 0; line-height: 1.9; color: #333; font-size: 15px;"><span style="color: #4facfe; font-weight: bold; margin-right: 8px;">01</span>æ–°é—»å†…å®¹</p>
</div>
</section>

<section style="margin-top: 40px; padding: 25px; background: linear-gradient(135deg, #fa709a 0%, #fee140 100%); border-radius: 15px;">
<p style="margin: 0 0 12px 0; font-size: 16px; font-weight: bold; color: #fff; letter-spacing: 2px;">ã€ å¾® è¯­ ã€‘</p>
<p style="margin: 0; color: #fff; font-size: 15px; line-height: 1.8; text-align: justify;">å¾®è¯­å†…å®¹</p>
</section>

</section>

è¦æ±‚ï¼šä½¿ç”¨ä¸Šè¿°åˆ†ç±»åçš„çœŸå®æ–°é—»ï¼Œæ¯ç±»5æ¡ï¼Œ1-2å¥è¯æ¦‚æ‹¬ï¼Œç”ŸæˆåŠ±å¿—å¾®è¯­ï¼Œåªè¾“å‡ºHTMLã€‚æ³¨æ„ï¼šæ–°é—»å†…å®¹å¼€å¤´ä¸è¦æ ‡æ³¨æ¥æºåª’ä½“ã€‚"""

    return call_llm_api(prompt, max_tokens=3000)

def save_raw_news(news_items: List[Dict], categorized: Dict[str, List[Dict]], date_str: str):
    """ä¿å­˜åŸå§‹æ–°é—»æ•°æ®"""
    raw_file = os.path.join(WORK_DIR, f"raw_news_{date_str}.json")
    with open(raw_file, 'w', encoding='utf-8') as f:
        json.dump({
            "all_news": news_items,
            "categorized": categorized
        }, f, ensure_ascii=False, indent=2)
    log(f"åŸå§‹æ–°é—»å·²ä¿å­˜: {raw_file}")

def main():
    """ä¸»å‡½æ•°"""
    log("=" * 50)
    log("RSS æ–°é—»æ”¶é›†å¼€å§‹")

    # è®¡ç®—æ—¥æœŸ
    yesterday = datetime.now() - timedelta(days=1)
    yesterday_str = yesterday.strftime("%Yå¹´%mæœˆ%dæ—¥")
    today_str = datetime.now().strftime("%Y%m%d")

    # 1. æ”¶é›†æ‰€æœ‰ RSS æ–°é—»
    all_news = collect_all_news()

    # 2. ä½¿ç”¨ AI åˆ†ç±»
    categorized_news = classify_news_with_ai(all_news)

    # ä¿å­˜åŸå§‹æ•°æ®
    save_raw_news(all_news, categorized_news, today_str)

    # 3. æ ¼å¼åŒ–ä¸º HTML
    log("æ­£åœ¨æ ¼å¼åŒ–æ–°é—»...")
    html_content = format_news_to_html(categorized_news, yesterday_str)

    if not html_content:
        log("æ ¼å¼åŒ–å¤±è´¥")
        return None

    # æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
    html_content = html_content.strip()
    if html_content.startswith('```'):
        html_content = html_content.split('\n', 1)[-1]
        # ç§»é™¤è¯­è¨€æ ‡è®°å¦‚ ```html
        if html_content.startswith('html'):
            html_content = html_content[4:].lstrip()
    if html_content.endswith('```'):
        html_content = html_content.rsplit('\n', 1)[0]
    html_content = html_content.strip()

    # ä¿å­˜ HTML
    html_file = os.path.join(WORK_DIR, f"news_{today_str}.md")
    with open(html_file, 'w', encoding='utf-8') as f:
        f.write(html_content)
    log(f"HTML å·²ä¿å­˜: {html_file}")

    log("RSS æ–°é—»æ”¶é›†å®Œæˆ")
    log("=" * 50)

    return html_content

if __name__ == "__main__":
    main()
