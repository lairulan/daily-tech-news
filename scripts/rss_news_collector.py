#!/usr/bin/env python3
"""
åŸºäº RSS è®¢é˜…çš„æ–°é—»æ”¶é›†è„šæœ¬
ä½¿ç”¨ Python å†…ç½®åº“è§£æ RSSï¼Œæ— éœ€é¢å¤–ä¾èµ–
"""

import os
import sys
import json
import urllib.request
import urllib.error
import ssl
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from typing import List, Dict
import re
from email.utils import parsedate_to_datetime

# åˆ›å»º SSL ä¸Šä¸‹æ–‡ï¼ˆå¤„ç†è¯ä¹¦é—®é¢˜ï¼‰
ssl_context = ssl._create_unverified_context()

# é…ç½®
DOUBAO_API_KEY = os.environ.get("DOUBAO_API_KEY")
WORK_DIR = os.path.expanduser("~/.claude/skills/daily-tech-news")
LOG_FILE = os.path.join(WORK_DIR, "logs", "rss-news.log")

# æ£€æŸ¥ API Key
if not DOUBAO_API_KEY:
    print("é”™è¯¯: æœªè®¾ç½® DOUBAO_API_KEY ç¯å¢ƒå˜é‡")
    print("è¯·è¿è¡Œ: export DOUBAO_API_KEY='your-api-key'")
    sys.exit(1)

# RSS æºé…ç½®ï¼ˆæ‰€æœ‰æºç»Ÿä¸€æ”¶é›†ï¼‰
ALL_RSS_SOURCES = [
    {"name": "é‡å­ä½", "url": "https://www.qbitai.com/feed", "limit": 8},
    {"name": "æœºå™¨ä¹‹å¿ƒ", "url": "https://www.jiqizhixin.com/rss", "limit": 8},
    {"name": "36æ°ª", "url": "https://36kr.com/feed", "limit": 8},
    {"name": "è™å—…", "url": "https://www.huxiu.com/rss/0.xml", "limit": 8},
    {"name": "é’›åª’ä½“", "url": "https://www.tmtpost.com/rss", "limit": 5},
    {"name": "çˆ±èŒƒå„¿", "url": "https://www.ifanr.com/feed", "limit": 5},
    {"name": "TechCrunch", "url": "https://techcrunch.com/feed/", "limit": 5},
    {"name": "TechCrunch AI", "url": "https://techcrunch.com/category/artificial-intelligence/feed/", "limit": 5},
    {"name": "The Verge", "url": "https://www.theverge.com/rss/index.xml", "limit": 5},
    {"name": "The Verge AI", "url": "https://www.theverge.com/rss/ai-artificial-intelligence/index.xml", "limit": 5},
    {"name": "åå°”è¡—è§é—»", "url": "https://dedicated.wallstreetcn.com/rss.xml", "limit": 10},
    {"name": "Bloomberg Tech", "url": "https://feeds.bloomberg.com/markets/news.rss", "limit": 8},
]

# ç›®æ ‡åˆ†ç±»
CATEGORIES = ["AI é¢†åŸŸ", "ç§‘æŠ€åŠ¨æ€", "è´¢ç»è¦é—»"]

def log(message):
    """è®°å½•æ—¥å¿—"""
    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{timestamp}] {message}\n")
    print(message)

def fetch_rss_items(url: str, limit: int = 10, hours_ago: int = 48) -> List[Dict]:
    """è·å– RSS æ¡ç›®"""
    try:
        # è®¾ç½®ç”¨æˆ·ä»£ç†
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }

        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=30, context=ssl_context) as response:
            content = response.read()

        # è§£æ XML
        root = ET.fromstring(content)

        # RSS æ ¼å¼ï¼š//rss/channel/item æˆ– //feed/entry
        items = []
        namespaces = {'': ''}  # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ å‘½åç©ºé—´

        # å°è¯•ä¸åŒçš„ RSS/Atom æ ¼å¼
        item_elements = root.findall('.//item') or root.findall('.//{http://www.w3.org/2005/Atom}entry')

        # ç²¾ç¡®çš„æ—¶é—´è¿‡æ»¤ï¼šåªæ”¶é›†æ˜¨å¤©ä¸€æ•´å¤©çš„æ–°é—»
        now = datetime.now()
        yesterday_start = datetime(now.year, now.month, now.day) - timedelta(days=1)  # æ˜¨å¤© 00:00:00
        yesterday_end = datetime(now.year, now.month, now.day) - timedelta(seconds=1)  # æ˜¨å¤© 23:59:59

        # å¤‡ç”¨ï¼šå¦‚æœéœ€è¦æ›´å®½æ¾çš„æ—¶é—´çª—å£ï¼ˆè¿‡å»24å°æ—¶ï¼‰
        cutoff_time = datetime.now() - timedelta(hours=hours_ago)

        for elem in item_elements[:limit * 2]:
            item = {}

            # æ ‡é¢˜ - æ›´å¥å£®çš„è§£æ
            title_text = ''
            for title_path in ['title', '{http://www.w3.org/2005/Atom}title']:
                title_elem = elem.find(title_path)
                if title_elem is not None and title_elem.text:
                    title_text = title_elem.text.strip()
                    break
            item['title'] = title_text if title_text else 'æ— æ ‡é¢˜'

            # æè¿°/æ‘˜è¦
            desc_text = ''
            for desc_path in ['description', '{http://www.w3.org/2005/Atom}summary', 'content', '{http://www.w3.org/2005/Atom}content']:
                desc_elem = elem.find(desc_path)
                if desc_elem is not None and desc_elem.text:
                    desc_text = desc_elem.text
                    break
            # ç§»é™¤ HTML æ ‡ç­¾
            desc_text = re.sub('<[^<]+?>', '', desc_text)
            desc_text = desc_text.strip()
            item['summary'] = desc_text[:500] if desc_text else ''

            # é“¾æ¥
            link_text = ''
            for link_path in ['link', '{http://www.w3.org/2005/Atom}link']:
                link_elem = elem.find(link_path)
                if link_elem is not None:
                    # å°è¯•è·å– href å±æ€§æˆ–æ–‡æœ¬
                    link_text = link_elem.get('href', '') or (link_elem.text if link_elem.text else '')
                    if link_text:
                        break
            item['link'] = link_text

            # å‘å¸ƒæ—¶é—´
            pub_text = ''
            for date_path in ['pubDate', '{http://www.w3.org/2005/Atom}published', 'date']:
                date_elem = elem.find(date_path)
                if date_elem is not None and date_elem.text:
                    pub_text = date_elem.text
                    break
            item['published'] = pub_text

            # æ¥æº
            source_text = ''
            for source_path in ['.//title', './/{http://www.w3.org/2005/Atom}title']:
                source_elem = root.find(source_path)
                if source_elem is not None and source_elem.text:
                    source_text = source_elem.text
                    break
            item['source'] = source_text if source_text else 'æœªçŸ¥æ¥æº'

            # ç²¾ç¡®çš„æ—¶é—´æ£€æŸ¥ï¼šåªæ”¶é›†æ˜¨å¤©çš„æ–°é—»
            if item['published']:
                try:
                    pub_time = parsedate_to_datetime(item['published'])
                    # è½¬æ¢ä¸ºæœ¬åœ°æ—¶åŒºï¼ˆåŒ—äº¬æ—¶é—´ï¼‰è¿›è¡Œæ¯”è¾ƒ
                    pub_time_local = pub_time.astimezone()

                    # æå–æ—¥æœŸéƒ¨åˆ†è¿›è¡Œæ¯”è¾ƒï¼ˆå¿½ç•¥å…·ä½“æ—¶é—´ï¼‰
                    pub_date = pub_time_local.date()
                    yesterday_date = yesterday_start.date()

                    # åªä¿ç•™æ˜¨å¤©çš„æ–°é—»
                    if pub_date != yesterday_date:
                        continue

                    # è®°å½•æ–°é—»çš„å‘å¸ƒæ—¶é—´ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                    item['parsed_time'] = pub_time_local.strftime("%Y-%m-%d %H:%M:%S")
                except:
                    # æ— æ³•è§£ææ—¶é—´ï¼Œä½¿ç”¨å®½æ¾çš„æ—¶é—´çª—å£ï¼ˆè¿‡å»24å°æ—¶ï¼‰
                    # è¿™æ ·å¯ä»¥ç¡®ä¿ä¸ä¼šé—æ¼é‡è¦æ–°é—»
                    pass

            items.append(item)

        return items[:limit]

    except Exception as e:
        log(f"è·å– RSS å¤±è´¥ [{url}]: {e}")
        return []

def collect_all_news() -> List[Dict]:
    """æ”¶é›†æ‰€æœ‰ RSS æ–°é—»åˆ°ä¸€èµ·"""
    all_items = []

    # è®¡ç®—æ—¶é—´èŒƒå›´ç”¨äºæ—¥å¿—
    now = datetime.now()
    yesterday_start = datetime(now.year, now.month, now.day) - timedelta(days=1)
    yesterday_end = datetime(now.year, now.month, now.day) - timedelta(seconds=1)

    log("å¼€å§‹æ”¶é›† RSS æ–°é—»...")
    log(f"æ—¶é—´è¿‡æ»¤èŒƒå›´: {yesterday_start.strftime('%Y-%m-%d %H:%M:%S')} åˆ° {yesterday_end.strftime('%Y-%m-%d %H:%M:%S')}")

    for source in ALL_RSS_SOURCES:
        log(f"  - {source['name']}")
        items = fetch_rss_items(source['url'], source['limit'])
        for item in items:
            item['rss_source'] = source['name']
        all_items.extend(items)
        log(f"    è·å– {len(items)} æ¡")

    # å»é‡ï¼ˆåŸºäºæ ‡é¢˜ï¼‰
    seen_titles = set()
    unique_items = []
    for item in all_items:
        title_lower = item['title'].lower()
        if title_lower not in seen_titles and item['title'] != 'æ— æ ‡é¢˜':
            seen_titles.add(title_lower)
            unique_items.append(item)

    log(f"æ”¶é›†å®Œæˆï¼Œå…±è·å– {len(unique_items)} æ¡å»é‡åæ–°é—»")
    return unique_items

def classify_news_with_ai(news_items: List[Dict]) -> Dict[str, List[Dict]]:
    """ä½¿ç”¨ AI å°†æ–°é—»åˆ†ç±»åˆ° 3 ä¸ªç±»åˆ«"""
    log("æ­£åœ¨ä½¿ç”¨ AI åˆ†ç±»æ–°é—»...")

    # å‡†å¤‡æ–°é—»åˆ—è¡¨ï¼ˆæœ€å¤š30æ¡ï¼Œé¿å… token è¿‡å¤šï¼‰
    news_list = news_items[:30]

    # æ„å»ºåˆ†ç±» prompt
    news_text = ""
    for i, item in enumerate(news_list, 1):
        news_text += f"{i}. æ ‡é¢˜: {item['title']}\n"
        if item['summary']:
            news_text += f"   æ‘˜è¦: {item['summary'][:100]}\n"
        news_text += f"   æ¥æº: {item['rss_source']}\n\n"

    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šæ–°é—»ç¼–è¾‘ã€‚è¯·å°†ä»¥ä¸‹æ–°é—»ä¸¥æ ¼åˆ†ç±»åˆ° 3 ä¸ªç±»åˆ«ä¸­ï¼š

{news_text}

åˆ†ç±»æ ‡å‡†ï¼š
- **AI é¢†åŸŸ**: äººå·¥æ™ºèƒ½ã€å¤§æ¨¡å‹ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ã€æœºå™¨äººã€AIåº”ç”¨ç­‰
- **ç§‘æŠ€åŠ¨æ€**: æ™ºèƒ½æ‰‹æœºã€ç”µè„‘ã€èŠ¯ç‰‡ã€äº’è”ç½‘ã€è½¯ä»¶ã€æ¸¸æˆã€æ–°èƒ½æºè½¦ã€èˆªå¤©ã€5G/6Gã€åˆ›ä¸šå…¬å¸ã€äº§å“å‘å¸ƒç­‰ï¼ˆéAIï¼‰
- **è´¢ç»è¦é—»**: è‚¡å¸‚ã€ç»æµã€è´§å¸æ”¿ç­–ã€èèµ„ã€å¹¶è´­ã€IPOã€é‡‘èæ”¿ç­–ã€å®è§‚ç»æµã€ä¼ä¸šè´¢æŠ¥ç­‰

è¯·æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼ˆåªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{
  "AI é¢†åŸŸ": [1, 3, 5, 7, 9],
  "ç§‘æŠ€åŠ¨æ€": [2, 4, 6, 8, 10],
  "è´¢ç»è¦é—»": [11, 12, 13, 14, 15]
}}

æ³¨æ„ï¼š
1. ä¸¥æ ¼æŒ‰ç…§åˆ†ç±»æ ‡å‡†ï¼Œä¸è¦æ··æ·†
2. æ¯ä¸ªç±»åˆ«é€‰æ‹©æœ€é‡è¦çš„ 5 æ¡
3. è¾“å‡ºçº¯ JSON æ ¼å¼"""

    result = call_doubao_api(prompt, max_tokens=2000)
    if not result:
        log("AI åˆ†ç±»å¤±è´¥")
        return {"AI é¢†åŸŸ": [], "ç§‘æŠ€åŠ¨æ€": [], "è´¢ç»è¦é—»": []}

    # è§£æ AI è¿”å›çš„ JSON
    try:
        # æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
        result = result.strip()
        if result.startswith('```'):
            result = result.split('\n', 1)[-1]
        if result.endswith('```'):
            result = result.rsplit('\n', 1)[0]
        result = result.strip()

        classification = json.loads(result)

        # æŒ‰åˆ†ç±»ç»„ç»‡æ–°é—»
        categorized = {cat: [] for cat in CATEGORIES}

        for category, indices in classification.items():
            if category in CATEGORIES:
                for idx in indices[:5]:  # æ¯ç±»æœ€å¤š 5 æ¡
                    if idx - 1 < len(news_list):
                        categorized[category].append(news_list[idx - 1])

        log(f"AI åˆ†ç±»å®Œæˆ: AIé¢†åŸŸ{len(categorized['AI é¢†åŸŸ'])}æ¡, ç§‘æŠ€åŠ¨æ€{len(categorized['ç§‘æŠ€åŠ¨æ€'])}æ¡, è´¢ç»è¦é—»{len(categorized['è´¢ç»è¦é—»'])}æ¡")
        return categorized

    except json.JSONDecodeError as e:
        log(f"è§£æ AI åˆ†ç±»ç»“æœå¤±è´¥: {e}")
        log(f"åŸå§‹ç»“æœ: {result[:500]}")
        return {"AI é¢†åŸŸ": [], "ç§‘æŠ€åŠ¨æ€": [], "è´¢ç»è¦é—»": []}

def call_doubao_api(prompt: str, max_tokens: int = 4000) -> str:
    """è°ƒç”¨è±†åŒ… API"""
    url = "https://ark.cn-beijing.volces.com/api/v3/chat/completions"

    payload = {
        "model": "doubao-seed-1-6-lite-251015",
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "max_tokens": max_tokens,
        "temperature": 0.7
    }

    try:
        data = json.dumps(payload).encode('utf-8')
        req = urllib.request.Request(
            url,
            data=data,
            headers={
                'Authorization': f"Bearer {DOUBAO_API_KEY}",
                'Content-Type': 'application/json'
            }
        )

        with urllib.request.urlopen(req, timeout=120, context=ssl_context) as response:
            result = json.loads(response.read().decode('utf-8'))
            return result["choices"][0]["message"]["content"]

    except Exception as e:
        log(f"è±†åŒ… API è°ƒç”¨å¤±è´¥: {e}")
        return None

def format_news_to_html(categorized_news: Dict[str, List[Dict]], yesterday: str) -> str:
    """å°†åˆ†ç±»åçš„æ–°é—»æ ¼å¼åŒ–ä¸º HTML"""
    # å‡†å¤‡æ–°é—»æ‘˜è¦
    news_summary = f"ä»¥ä¸‹æ˜¯{yesterday}é€šè¿‡ RSS æ”¶é›†å¹¶åˆ†ç±»çš„æ–°é—»ï¼š\n\n"

    for category in CATEGORIES:
        items = categorized_news.get(category, [])
        news_summary += f"\n## {category}\n"
        for i, item in enumerate(items[:5], 1):
            title = item['title'][:100]
            news_summary += f"{i}. {title}\n"

    # æ„å»º prompt
    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šæ–°é—»ç¼–è¾‘ã€‚ä»¥ä¸‹æ˜¯{yesterday}é€šè¿‡ RSS æ”¶é›†å¹¶åˆ†ç±»çš„æ–°é—»ï¼š

{news_summary}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆåªè¾“å‡º HTMLï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼‰ï¼š

<section style="padding: 20px; font-family: -apple-system, BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', sans-serif;">

<section style="text-align: center; padding: 20px 0 30px 0; background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%); border-radius: 15px; margin-bottom: 30px;">
<p style="margin: 0; font-size: 14px; color: #666; letter-spacing: 1px;">å†œå†ä¹™å·³å¹´XXæœˆXX</p>
<p style="margin: 8px 0 0 0; font-size: 20px; font-weight: bold; color: #333; letter-spacing: 3px;">æ˜ŸæœŸX</p>
<p style="margin: 8px 0 0 0; font-size: 13px; color: #999;">2026å¹´XæœˆXæ—¥</p>
</section>

<section style="margin-bottom: 30px;">
<p style="display: inline-block; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: #fff; font-size: 18px; font-weight: bold; padding: 10px 25px; border-radius: 25px; margin: 0 0 20px 0; box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);">ğŸ“± AI é¢†åŸŸ</p>
<div style="padding: 0 10px;">
<p style="margin: 0 0 15px 0; line-height: 1.9; color: #333; font-size: 15px;"><span style="color: #667eea; font-weight: bold; margin-right: 8px;">01</span>æ–°é—»å†…å®¹</p>
</div>
</section>

<section style="margin-bottom: 30px;">
<p style="display: inline-block; background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: #fff; font-size: 18px; font-weight: bold; padding: 10px 25px; border-radius: 25px; margin: 0 0 20px 0; box-shadow: 0 4px 15px rgba(245, 87, 108, 0.3);">ğŸ’» ç§‘æŠ€åŠ¨æ€</p>
<div style="padding: 0 10px;">
<p style="margin: 0 0 15px 0; line-height: 1.9; color: #333; font-size: 15px;"><span style="color: #f5576c; font-weight: bold; margin-right: 8px;">01</span>æ–°é—»å†…å®¹</p>
</div>
</section>

<section style="margin-bottom: 30px;">
<p style="display: inline-block; background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); color: #fff; font-size: 18px; font-weight: bold; padding: 10px 25px; border-radius: 25px; margin: 0 0 20px 0; box-shadow: 0 4px 15px rgba(79, 172, 254, 0.3);">ğŸ’° è´¢ç»è¦é—»</p>
<div style="padding: 0 10px;">
<p style="margin: 0 0 15px 0; line-height: 1.9; color: #333; font-size: 15px;"><span style="color: #4facfe; font-weight: bold; margin-right: 8px;">01</span>æ–°é—»å†…å®¹</p>
</div>
</section>

<section style="margin-top: 40px; padding: 25px; background: linear-gradient(135deg, #fa709a 0%, #fee140 100%); border-radius: 15px;">
<p style="margin: 0 0 12px 0; font-size: 16px; font-weight: bold; color: #fff; letter-spacing: 2px;">ã€ å¾® è¯­ ã€‘</p>
<p style="margin: 0; color: #fff; font-size: 15px; line-height: 1.8; text-align: justify;">å¾®è¯­å†…å®¹</p>
</section>

</section>

è¦æ±‚ï¼šä½¿ç”¨ä¸Šè¿°åˆ†ç±»åçš„çœŸå®æ–°é—»ï¼Œæ¯ç±»5æ¡ï¼Œ1-2å¥è¯æ¦‚æ‹¬ï¼Œç”ŸæˆåŠ±å¿—å¾®è¯­ï¼Œåªè¾“å‡ºHTMLã€‚æ³¨æ„ï¼šæ–°é—»å†…å®¹å¼€å¤´ä¸è¦æ ‡æ³¨æ¥æºåª’ä½“ã€‚"""

    return call_doubao_api(prompt, max_tokens=3000)

def save_raw_news(news_items: List[Dict], categorized: Dict[str, List[Dict]], date_str: str):
    """ä¿å­˜åŸå§‹æ–°é—»æ•°æ®"""
    raw_file = os.path.join(WORK_DIR, f"raw_news_{date_str}.json")
    with open(raw_file, 'w', encoding='utf-8') as f:
        json.dump({
            "all_news": news_items,
            "categorized": categorized
        }, f, ensure_ascii=False, indent=2)
    log(f"åŸå§‹æ–°é—»å·²ä¿å­˜: {raw_file}")

def main():
    """ä¸»å‡½æ•°"""
    log("=" * 50)
    log("RSS æ–°é—»æ”¶é›†å¼€å§‹")

    # è®¡ç®—æ—¥æœŸ
    yesterday = datetime.now() - timedelta(days=1)
    yesterday_str = yesterday.strftime("%Yå¹´%mæœˆ%dæ—¥")
    today_str = datetime.now().strftime("%Y%m%d")

    # 1. æ”¶é›†æ‰€æœ‰ RSS æ–°é—»
    all_news = collect_all_news()

    # 2. ä½¿ç”¨ AI åˆ†ç±»
    categorized_news = classify_news_with_ai(all_news)

    # ä¿å­˜åŸå§‹æ•°æ®
    save_raw_news(all_news, categorized_news, today_str)

    # 3. æ ¼å¼åŒ–ä¸º HTML
    log("æ­£åœ¨æ ¼å¼åŒ–æ–°é—»...")
    html_content = format_news_to_html(categorized_news, yesterday_str)

    if not html_content:
        log("æ ¼å¼åŒ–å¤±è´¥")
        return None

    # æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
    html_content = html_content.strip()
    if html_content.startswith('```'):
        html_content = html_content.split('\n', 1)[-1]
        # ç§»é™¤è¯­è¨€æ ‡è®°å¦‚ ```html
        if html_content.startswith('html'):
            html_content = html_content[4:].lstrip()
    if html_content.endswith('```'):
        html_content = html_content.rsplit('\n', 1)[0]
    html_content = html_content.strip()

    # ä¿å­˜ HTML
    html_file = os.path.join(WORK_DIR, f"news_{today_str}.md")
    with open(html_file, 'w', encoding='utf-8') as f:
        f.write(html_content)
    log(f"HTML å·²ä¿å­˜: {html_file}")

    log("RSS æ–°é—»æ”¶é›†å®Œæˆ")
    log("=" * 50)

    return html_content

if __name__ == "__main__":
    main()
